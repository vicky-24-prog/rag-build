{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab58180",
   "metadata": {},
   "source": [
    "# RAG Evaluation Analysis\n",
    "**Production-quality evaluation dashboard for the FAISS + Gemini RAG pipeline.**\n",
    "\n",
    "Sections:\n",
    "1. Install & Import Dependencies\n",
    "2. Configuration & Synthetic Data (for offline demo)\n",
    "3. Load Evaluation Results from SQLite\n",
    "4. Faithfulness Score Distribution Histogram\n",
    "5. Query Route Assignment Pie Chart\n",
    "6. Retrieval Metrics (Recall@k, MRR, NDCG@k)\n",
    "7. Latency & Cost Breakdown\n",
    "8. Top-10 Failure Analysis (worst faithfulness)\n",
    "9. Top-3 Failure Pattern Clustering (via Gemini-Flash)\n",
    "10. Experiment Comparison Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Install & Import Dependencies ──────────────────────────────────────\n",
    "# Uncomment the line below if running for the first time\n",
    "# !pip install faiss-cpu sentence-transformers google-generativeai pandas matplotlib seaborn numpy\n",
    "\n",
    "import sys, os, json, sqlite3, csv, time, logging, math, uuid, warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# Add project root to path so we can import rag.*\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "print(\"✓ Imports OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e2579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2. Configuration ──────────────────────────────────────────────────────\n",
    "@dataclass\n",
    "class Config:\n",
    "    gemini_flash_model: str   = \"gemini-1.5-flash\"\n",
    "    gemini_pro_model: str     = \"gemini-1.5-pro\"\n",
    "    embedding_model: str      = \"all-mpnet-base-v2\"\n",
    "    k_retrieved: int          = 5\n",
    "    db_path: str              = \"../evaluations.db\"\n",
    "    experiments_dir: str      = \"../experiments\"\n",
    "    routing_log: str          = \"../routing_log.jsonl\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Load Gemini API key (set GEMINI_API_KEY as an env var before running)\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "_gemini_available = bool(GEMINI_API_KEY)\n",
    "\n",
    "if _gemini_available:\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=GEMINI_API_KEY)\n",
    "        flash_llm = genai.GenerativeModel(cfg.gemini_flash_model)\n",
    "        pro_llm   = genai.GenerativeModel(cfg.gemini_pro_model)\n",
    "        print(f\"✓ Gemini initialised  (flash={cfg.gemini_flash_model}, pro={cfg.gemini_pro_model})\")\n",
    "    except Exception as e:\n",
    "        _gemini_available = False\n",
    "        print(f\"⚠  Gemini init failed: {e}. Pattern analysis will be skipped.\")\n",
    "else:\n",
    "    print(\"⚠  GEMINI_API_KEY not set. Gemini-dependent cells will use cached/synthetic data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d934a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3. Load Evaluation Results ────────────────────────────────────────────\n",
    "# Tries to load from real SQLite DB + routing JSONL.\n",
    "# Falls back to realistic synthetic data so the notebook is always runnable.\n",
    "\n",
    "def load_evaluation_data(db_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load evaluation rows from SQLite → DataFrame.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        df = pd.read_sql_query(\"SELECT * FROM evaluations ORDER BY timestamp\", conn)\n",
    "        conn.close()\n",
    "        if len(df) > 0:\n",
    "            print(f\"✓ Loaded {len(df)} real evaluation rows from {db_path}\")\n",
    "            return df\n",
    "    except Exception as e:\n",
    "        print(f\"  SQLite load failed ({e}). Generating synthetic data.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_routing_data(log_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load routing decisions from JSONL → DataFrame.\"\"\"\n",
    "    records = []\n",
    "    try:\n",
    "        with open(log_path) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    records.append(json.loads(line))\n",
    "        if records:\n",
    "            print(f\"✓ Loaded {len(records)} routing decisions from {log_path}\")\n",
    "            return pd.DataFrame(records)\n",
    "    except Exception as e:\n",
    "        print(f\"  Routing log load failed ({e}). Using synthetic routing data.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _synthetic_evals(n: int = 120, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"Generate realistic synthetic evaluation rows for demo.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    routes = rng.choice(\n",
    "        [\"RAG_RETRIEVAL\", \"DIRECT_LLM\", \"HYBRID\"],\n",
    "        size=n,\n",
    "        p=[0.58, 0.25, 0.17],\n",
    "    )\n",
    "    # RAG queries tend to have lower faithfulness (harder)\n",
    "    base_faith = np.where(routes == \"DIRECT_LLM\", 0.88, np.where(routes == \"HYBRID\", 0.75, 0.72))\n",
    "    faithfulness = np.clip(rng.normal(base_faith, 0.12), 0, 1)\n",
    "    answer_relevance = np.clip(rng.normal(0.82, 0.11), 0, 1)\n",
    "    context_precision = np.clip(rng.normal(0.74, 0.15), 0, 1)\n",
    "    context_recall = np.clip(rng.normal(0.69, 0.14), 0, 1)\n",
    "\n",
    "    sample_queries = [\n",
    "        \"Recommend running shoes under 3000 rupees\",\n",
    "        \"Best wireless headphones for gym\",\n",
    "        \"What is machine learning?\",\n",
    "        \"Is our return policy competitive?\",\n",
    "        \"Show me gaming laptops with RTX GPU\",\n",
    "        \"What is RAG in NLP?\",\n",
    "        \"Compare our laptop warranty to industry standard\",\n",
    "        \"Noise cancelling earbuds under 2000\",\n",
    "        \"What is gradient descent?\",\n",
    "        \"Best rated products this month\",\n",
    "    ] * (n // 10 + 1)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"query_id\": [str(uuid.uuid4()) for _ in range(n)],\n",
    "        \"query\": sample_queries[:n],\n",
    "        \"response\": [\"[synthetic response]\"] * n,\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevance\": answer_relevance,\n",
    "        \"context_precision\": context_precision,\n",
    "        \"context_recall\": context_recall,\n",
    "        \"judge_model\": [\"gemini-1.5-pro\"] * n,\n",
    "        \"timestamp\": np.linspace(time.time() - 7200, time.time(), n),\n",
    "        \"latency_ms\": rng.normal(2800, 400, n),\n",
    "        \"route\": routes,\n",
    "        \"routing_method\": rng.choice([\"embedding\", \"llm\"], size=n, p=[0.72, 0.28]),\n",
    "        \"routing_confidence\": np.clip(rng.normal(0.82, 0.10), 0.4, 1.0),\n",
    "        \"retrieval_latency_ms\": rng.normal(28, 8, n),\n",
    "        \"generation_latency_ms\": rng.normal(900, 150, n),\n",
    "        \"evaluation_latency_ms\": rng.normal(2100, 300, n),\n",
    "        \"estimated_cost_usd\": rng.uniform(0.005, 0.015, n),\n",
    "        \"input_tokens\": rng.integers(600, 1400, n),\n",
    "        \"output_tokens\": rng.integers(80, 250, n),\n",
    "        \"unsupported_claims\": [\n",
    "            json.dumps([\"Hallucinated price\", \"Unverified brand claim\"]\n",
    "                      if f < 0.5 else ([\"Minor unsupported claim\"] if f < 0.75 else []))\n",
    "            for f in faithfulness\n",
    "        ],\n",
    "        \"missing_facts\": [json.dumps([]) for _ in range(n)],\n",
    "        \"relevance_reasoning\": [\"[synthetic reasoning]\"] * n,\n",
    "    })\n",
    "\n",
    "\n",
    "def _synthetic_routing(n: int = 120, seed: int = 42) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    routes = rng.choice(\n",
    "        [\"RAG_RETRIEVAL\", \"DIRECT_LLM\", \"HYBRID\"],\n",
    "        size=n,\n",
    "        p=[0.58, 0.25, 0.17],\n",
    "    )\n",
    "    return pd.DataFrame({\n",
    "        \"route\": routes,\n",
    "        \"method\": rng.choice([\"embedding\", \"llm\"], size=n, p=[0.72, 0.28]),\n",
    "        \"confidence\": np.clip(rng.normal(0.82, 0.10, n), 0.4, 1.0),\n",
    "        \"latency_ms\": np.where(\n",
    "            rng.choice([\"embedding\", \"llm\"], size=n, p=[0.72, 0.28]) == \"embedding\",\n",
    "            rng.normal(6, 2, n),\n",
    "            rng.normal(210, 40, n),\n",
    "        ),\n",
    "    })\n",
    "\n",
    "\n",
    "# --- Load or generate data ---\n",
    "df_eval = load_evaluation_data(cfg.db_path)\n",
    "if df_eval is None:\n",
    "    df_eval = _synthetic_evals()\n",
    "    print(f\"  Using {len(df_eval)} synthetic evaluation rows.\")\n",
    "\n",
    "df_route = load_routing_data(cfg.routing_log)\n",
    "if df_route is None:\n",
    "    df_route = _synthetic_routing(len(df_eval))\n",
    "    print(f\"  Using {len(df_route)} synthetic routing rows.\")\n",
    "\n",
    "# Parse JSON columns if loaded from SQLite\n",
    "for col in [\"unsupported_claims\", \"missing_facts\"]:\n",
    "    if col in df_eval.columns:\n",
    "        df_eval[col] = df_eval[col].apply(\n",
    "            lambda x: json.loads(x) if isinstance(x, str) else (x or [])\n",
    "        )\n",
    "\n",
    "df_eval.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4818860",
   "metadata": {},
   "source": [
    "## 4. Faithfulness Score Distribution\n",
    "\n",
    "Faithfulness measures what fraction of the generated response's claims are **directly supported by retrieved context**.  \n",
    "A score of 1.0 means every claim is grounded; 0.0 means the response is entirely hallucinated.\n",
    "\n",
    "**Thresholds used:**\n",
    "- ≥ 0.80 → High quality (green zone)\n",
    "- 0.50 – 0.80 → Medium quality (amber zone)\n",
    "- < 0.50 → Low quality / hallucination risk (red zone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e82047",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "scores = df_eval[\"faithfulness\"].dropna()\n",
    "mean_f = scores.mean()\n",
    "median_f = scores.median()\n",
    "\n",
    "# Coloured zone backgrounds\n",
    "ax.axvspan(0.0, 0.5,  alpha=0.08, color=\"red\",    label=\"_nolegend_\")\n",
    "ax.axvspan(0.5, 0.8,  alpha=0.08, color=\"orange\",  label=\"_nolegend_\")\n",
    "ax.axvspan(0.8, 1.0,  alpha=0.08, color=\"green\",   label=\"_nolegend_\")\n",
    "\n",
    "# Histogram\n",
    "n_bins = 25\n",
    "ax.hist(scores, bins=n_bins, color=\"#4C72B0\", edgecolor=\"white\", linewidth=0.6,\n",
    "        alpha=0.85, density=True, label=\"Frequency (density)\")\n",
    "\n",
    "# KDE overlay\n",
    "try:\n",
    "    from scipy.stats import gaussian_kde\n",
    "    kde = gaussian_kde(scores, bw_method=0.25)\n",
    "    x_line = np.linspace(0, 1, 300)\n",
    "    ax.plot(x_line, kde(x_line), color=\"#DD8452\", linewidth=2.5, label=\"KDE\")\n",
    "except ImportError:\n",
    "    pass  # scipy optional\n",
    "\n",
    "# Mean / Median lines\n",
    "ax.axvline(mean_f,   color=\"#C44E52\", linewidth=2, linestyle=\"--\", label=f\"Mean  = {mean_f:.3f}\")\n",
    "ax.axvline(median_f, color=\"#55A868\", linewidth=2, linestyle=\":\",  label=f\"Median = {median_f:.3f}\")\n",
    "\n",
    "# Zone annotations\n",
    "pct_high   = (scores >= 0.8).mean() * 100\n",
    "pct_medium = ((scores >= 0.5) & (scores < 0.8)).mean() * 100\n",
    "pct_low    = (scores < 0.5).mean() * 100\n",
    "\n",
    "for x_pos, pct, label, color in [\n",
    "    (0.25, pct_low,    f\"Low\\n{pct_low:.1f}%\",    \"red\"),\n",
    "    (0.65, pct_medium, f\"Medium\\n{pct_medium:.1f}%\", \"darkorange\"),\n",
    "    (0.90, pct_high,   f\"High\\n{pct_high:.1f}%\",  \"darkgreen\"),\n",
    "]:\n",
    "    ax.text(x_pos, ax.get_ylim()[1] * 0.92 if ax.get_ylim()[1] > 0 else 2.5,\n",
    "            label, ha=\"center\", fontsize=11, color=color, fontweight=\"bold\")\n",
    "\n",
    "ax.set_xlabel(\"Faithfulness Score\", fontsize=13)\n",
    "ax.set_ylabel(\"Density\", fontsize=13)\n",
    "ax.set_title(\"Faithfulness Score Distribution\\n(LLM-as-Judge, Gemini-1.5-Pro)\", fontsize=14)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"faithfulness_distribution.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(scores.describe().round(4).to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ff71b",
   "metadata": {},
   "source": [
    "## 5. Query Route Assignment\n",
    "\n",
    "The `QueryRouter` classifies each query into one of three routes before retrieval.  \n",
    "The pie chart on the left shows the **distribution of routes**, while the bar chart on the right  \n",
    "shows how **mean faithfulness varies by route** — a key signal for whether routing improves quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_col = \"route\" if \"route\" in df_eval.columns else None\n",
    "if route_col is None and \"route\" in df_route.columns:\n",
    "    # Merge routing data into eval df on index (best-effort)\n",
    "    df_eval = df_eval.copy()\n",
    "    df_eval[\"route\"] = df_route[\"route\"].values[:len(df_eval)]\n",
    "    route_col = \"route\"\n",
    "\n",
    "route_counts = df_eval[route_col].value_counts() if route_col else df_route[\"route\"].value_counts()\n",
    "route_order  = [\"RAG_RETRIEVAL\", \"DIRECT_LLM\", \"HYBRID\"]\n",
    "route_counts = route_counts.reindex(\n",
    "    [r for r in route_order if r in route_counts.index], fill_value=0\n",
    ")\n",
    "\n",
    "colors = [\"#4C72B0\", \"#55A868\", \"#DD8452\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# --- Pie chart ---\n",
    "wedges, texts, autotexts = axes[0].pie(\n",
    "    route_counts.values,\n",
    "    labels=route_counts.index,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=colors,\n",
    "    startangle=90,\n",
    "    pctdistance=0.75,\n",
    "    wedgeprops={\"edgecolor\": \"white\", \"linewidth\": 2},\n",
    ")\n",
    "for at in autotexts:\n",
    "    at.set_fontsize(11)\n",
    "    at.set_fontweight(\"bold\")\n",
    "axes[0].set_title(\"Query Route Distribution\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Add routing method annotation\n",
    "if \"method\" in df_route.columns:\n",
    "    method_counts = df_route[\"method\"].value_counts()\n",
    "    legend_labels = [f\"{m}: {c} ({c/len(df_route)*100:.0f}%)\"\n",
    "                     for m, c in method_counts.items()]\n",
    "    axes[0].legend(legend_labels, title=\"Routing method\", loc=\"lower left\",\n",
    "                   fontsize=9, title_fontsize=9)\n",
    "\n",
    "# --- Bar: mean faithfulness per route ---\n",
    "if route_col and \"faithfulness\" in df_eval.columns:\n",
    "    faith_by_route = (\n",
    "        df_eval.groupby(route_col)[\"faithfulness\"]\n",
    "        .agg([\"mean\", \"std\", \"count\"])\n",
    "        .reindex([r for r in route_order if r in df_eval[route_col].unique()])\n",
    "    )\n",
    "    bars = axes[1].bar(\n",
    "        faith_by_route.index,\n",
    "        faith_by_route[\"mean\"],\n",
    "        color=colors[:len(faith_by_route)],\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.85,\n",
    "        yerr=faith_by_route[\"std\"],\n",
    "        capsize=5,\n",
    "    )\n",
    "    for bar, (_, row) in zip(bars, faith_by_route.iterrows()):\n",
    "        axes[1].text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.02,\n",
    "            f\"{row['mean']:.3f}\\n(n={int(row['count'])})\",\n",
    "            ha=\"center\", va=\"bottom\", fontsize=10, fontweight=\"bold\"\n",
    "        )\n",
    "    axes[1].set_ylim(0, 1.15)\n",
    "    axes[1].set_ylabel(\"Mean Faithfulness ± std\", fontsize=12)\n",
    "    axes[1].set_xlabel(\"Route\", fontsize=12)\n",
    "    axes[1].set_title(\"Mean Faithfulness by Route\", fontsize=13, fontweight=\"bold\")\n",
    "    axes[1].axhline(df_eval[\"faithfulness\"].mean(), color=\"gray\", linestyle=\"--\",\n",
    "                    linewidth=1.2, label=f\"Overall mean = {df_eval['faithfulness'].mean():.3f}\")\n",
    "    axes[1].legend(fontsize=10)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"No faithfulness data to plot\",\n",
    "                 ha=\"center\", va=\"center\", fontsize=12, transform=axes[1].transAxes)\n",
    "\n",
    "plt.suptitle(\"Query Routing Analysis\", fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"routing_distribution.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRoute counts:\")\n",
    "print(route_counts.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698f00a",
   "metadata": {},
   "source": [
    "## 6. Retrieval Metrics — Recall@k, MRR, NDCG@k\n",
    "\n",
    "If an experiments directory exists, load the aggregated metrics from the latest run.  \n",
    "Otherwise, display a sample results table using the current e-commerce baseline metrics  \n",
    "(Recall@5 = 0.85, MRR = 0.82 as measured by the existing evaluation layer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_experiment_metrics(experiments_dir: str) -> Optional[dict]:\n",
    "    \"\"\"Try loading the most recently modified experiment's results.json.\"\"\"\n",
    "    base = Path(experiments_dir)\n",
    "    if not base.exists():\n",
    "        return None\n",
    "    results_files = sorted(base.glob(\"*/results.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not results_files:\n",
    "        return None\n",
    "    with open(results_files[0]) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "exp_metrics = _load_experiment_metrics(cfg.experiments_dir)\n",
    "\n",
    "if exp_metrics:\n",
    "    print(f\"✓ Loaded metrics from experiment: {exp_metrics.get('experiment_id', '?')}\")\n",
    "    mrr_val = exp_metrics.get(\"mrr\", 0.82)\n",
    "    recall  = exp_metrics.get(\"mean_recall\", {})\n",
    "    ndcg    = exp_metrics.get(\"mean_ndcg\", {})\n",
    "else:\n",
    "    print(\"  No experiments found. Using sample metrics from existing baseline.\")\n",
    "    # Your existing baseline numbers\n",
    "    mrr_val = 0.82\n",
    "    recall  = {\"1\": 0.62, \"3\": 0.78, \"5\": 0.85, \"10\": 0.91}\n",
    "    ndcg    = {\"1\": 0.62, \"3\": 0.72, \"5\": 0.80,  \"10\": 0.85}\n",
    "\n",
    "# Normalise key type\n",
    "recall = {int(k): v for k, v in recall.items()}\n",
    "ndcg   = {int(k): v for k, v in ndcg.items()}\n",
    "\n",
    "k_vals = sorted(set(recall) | set(ndcg))\n",
    "\n",
    "# ── Table ──────────────────────────────────────────────────────────────────\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"k\":          k_vals,\n",
    "    \"Recall@k\":   [recall.get(k, 0) for k in k_vals],\n",
    "    \"NDCG@k\":     [ndcg.get(k, 0)   for k in k_vals],\n",
    "}).set_index(\"k\")\n",
    "\n",
    "print(f\"\\nMRR = {mrr_val:.4f}\\n\")\n",
    "print(metrics_df.round(4).to_string())\n",
    "\n",
    "# ── Bar chart ─────────────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "x = np.arange(len(k_vals))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, metrics_df[\"Recall@k\"], width,\n",
    "               label=\"Recall@k\", color=\"#4C72B0\", alpha=0.85, edgecolor=\"white\")\n",
    "bars2 = ax.bar(x + width/2, metrics_df[\"NDCG@k\"],   width,\n",
    "               label=\"NDCG@k\",   color=\"#55A868\", alpha=0.85, edgecolor=\"white\")\n",
    "\n",
    "ax.axhline(mrr_val, color=\"#DD8452\", linestyle=\"--\", linewidth=2,\n",
    "           label=f\"MRR = {mrr_val:.4f}\")\n",
    "\n",
    "for bar in list(bars1) + list(bars2):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f\"{bar.get_height():.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"k={k}\" for k in k_vals], fontsize=11)\n",
    "ax.set_ylim(0, 1.12)\n",
    "ax.set_ylabel(\"Score\", fontsize=12)\n",
    "ax.set_title(\"Retrieval Metrics: Recall@k and NDCG@k\", fontsize=13, fontweight=\"bold\")\n",
    "ax.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"retrieval_metrics.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878e8ad",
   "metadata": {},
   "source": [
    "## 7. Latency & Cost Breakdown\n",
    "\n",
    "Understanding where time is spent and what each evaluation costs per query helps  \n",
    "decide where to optimise (e.g., caching, smaller judge model, batch API calls).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ── Latency stacked bar (per query, mean) ─────────────────────────────────\n",
    "latency_cols = {\n",
    "    \"retrieval_latency_ms\":  \"Retrieval\",\n",
    "    \"generation_latency_ms\": \"Generation\",\n",
    "    \"evaluation_latency_ms\": \"LLM Judge\",\n",
    "}\n",
    "present_latency = {k: v for k, v in latency_cols.items() if k in df_eval.columns}\n",
    "\n",
    "if present_latency:\n",
    "    means = {label: df_eval[col].mean() for col, label in present_latency.items()}\n",
    "    pal = [\"#4C72B0\", \"#55A868\", \"#DD8452\"][:len(means)]\n",
    "\n",
    "    bottom = 0\n",
    "    for (label, val), color in zip(means.items(), pal):\n",
    "        axes[0].bar(\"Mean per query\", val, bottom=bottom, color=color, label=label,\n",
    "                    edgecolor=\"white\", linewidth=1.5, alpha=0.88)\n",
    "        axes[0].text(0, bottom + val / 2, f\"{val:.0f} ms\", ha=\"center\",\n",
    "                     va=\"center\", fontsize=11, color=\"white\", fontweight=\"bold\")\n",
    "        bottom += val\n",
    "\n",
    "    axes[0].set_ylabel(\"Latency (ms)\", fontsize=12)\n",
    "    axes[0].set_title(\"Mean Latency Breakdown (per query)\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0].legend(loc=\"upper right\", fontsize=10)\n",
    "    axes[0].set_ylim(0, bottom * 1.15)\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, \"No latency columns in data\", ha=\"center\", va=\"center\",\n",
    "                 fontsize=12, transform=axes[0].transAxes)\n",
    "\n",
    "# ── Cost distribution ─────────────────────────────────────────────────────\n",
    "if \"estimated_cost_usd\" in df_eval.columns:\n",
    "    cost_data = df_eval[\"estimated_cost_usd\"].dropna() * 100  # cents\n",
    "    axes[1].hist(cost_data, bins=20, color=\"#4C72B0\", edgecolor=\"white\",\n",
    "                 linewidth=0.6, alpha=0.85)\n",
    "    axes[1].axvline(cost_data.mean(), color=\"#C44E52\", linewidth=2, linestyle=\"--\",\n",
    "                    label=f\"Mean = {cost_data.mean():.3f} ¢\")\n",
    "    axes[1].set_xlabel(\"Estimated Cost per Query (¢ USD)\", fontsize=12)\n",
    "    axes[1].set_ylabel(\"Count\", fontsize=12)\n",
    "    axes[1].set_title(\"Cost Distribution (Judge API calls)\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1].legend(fontsize=10)\n",
    "\n",
    "    total_cost = df_eval[\"estimated_cost_usd\"].sum()\n",
    "    print(f\"\\nTotal estimated cost for {len(df_eval)} queries: ${total_cost:.4f} USD\")\n",
    "    if \"input_tokens\" in df_eval.columns:\n",
    "        print(f\"Total input tokens : {df_eval['input_tokens'].sum():,}\")\n",
    "        print(f\"Total output tokens: {df_eval['output_tokens'].sum():,}\")\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"No cost data available\", ha=\"center\", va=\"center\",\n",
    "                 fontsize=12, transform=axes[1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"latency_cost.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde1136",
   "metadata": {},
   "source": [
    "## 8. Top-10 Failure Analysis\n",
    "\n",
    "The lowest-faithfulness queries reveal systematic failure modes.  \n",
    "Each row shows the query, faithfulness score, and the specific unsupported claims  \n",
    "flagged by the LLM judge — these are claims in the response that are **not in the retrieved context**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst10 = df_eval.nsmallest(10, \"faithfulness\")[\n",
    "    [\"query\", \"faithfulness\", \"answer_relevance\", \"context_precision\", \"unsupported_claims\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Pretty-print unsupported_claims\n",
    "worst10[\"unsupported_claims_str\"] = worst10[\"unsupported_claims\"].apply(\n",
    "    lambda x: \"; \".join(x) if isinstance(x, list) else str(x)\n",
    ")\n",
    "\n",
    "display_df = worst10[[\"query\", \"faithfulness\", \"answer_relevance\", \"unsupported_claims_str\"]].copy()\n",
    "display_df.columns = [\"Query\", \"Faithfulness\", \"Relevance\", \"Unsupported Claims\"]\n",
    "display_df[\"Query\"] = display_df[\"Query\"].apply(lambda q: q[:70] + \"…\" if len(q) > 70 else q)\n",
    "\n",
    "try:\n",
    "    display(display_df.style\n",
    "        .background_gradient(subset=[\"Faithfulness\"], cmap=\"RdYlGn\", vmin=0, vmax=1)\n",
    "        .background_gradient(subset=[\"Relevance\"],   cmap=\"RdYlGn\", vmin=0, vmax=1)\n",
    "        .set_properties(**{\"font-size\": \"11px\"})\n",
    "        .set_caption(\"Top-10 Worst Queries by Faithfulness\")\n",
    "    )\n",
    "except Exception:\n",
    "    print(display_df.to_string(index=False))\n",
    "\n",
    "# Horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "bar_colors = [\"#C44E52\" if f < 0.5 else \"#DD8452\" for f in worst10[\"faithfulness\"]]\n",
    "ax.barh(\n",
    "    range(len(worst10)),\n",
    "    worst10[\"faithfulness\"],\n",
    "    color=bar_colors,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.8,\n",
    "    alpha=0.85,\n",
    ")\n",
    "ax.set_yticks(range(len(worst10)))\n",
    "ax.set_yticklabels(\n",
    "    [f\"Q{i+1}: {q[:45]}…\" if len(q) > 45 else f\"Q{i+1}: {q}\"\n",
    "     for i, q in enumerate(worst10[\"query\"])],\n",
    "    fontsize=9,\n",
    ")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel(\"Faithfulness Score\", fontsize=12)\n",
    "ax.set_title(\"Top-10 Lowest Faithfulness Queries\", fontsize=13, fontweight=\"bold\")\n",
    "ax.axvline(0.5, color=\"gray\", linestyle=\":\", linewidth=1.2, label=\"Threshold = 0.5\")\n",
    "ax.legend(fontsize=10)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"failure_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a1d5d",
   "metadata": {},
   "source": [
    "## 9. Top-3 Failure Pattern Clustering (via Gemini-Flash)\n",
    "\n",
    "The unsupported claims from the failure analysis are clustered by Gemini-Flash  \n",
    "into the three most common hallucination patterns. This surfaces actionable insight  \n",
    "without manual review of every failure.\n",
    "\n",
    "> **Requires** `GEMINI_API_KEY` to be set. If not available, hardcoded sample patterns are shown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9775c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_failure_patterns_from_gemini(claims_list: list) -> list:\n",
    "    \"\"\"Ask Gemini-Flash to cluster claims into 3 failure patterns.\"\"\"\n",
    "    flat_claims = []\n",
    "    for claims in claims_list:\n",
    "        if isinstance(claims, list):\n",
    "            flat_claims.extend(claims)\n",
    "        elif isinstance(claims, str) and claims:\n",
    "            flat_claims.append(claims)\n",
    "\n",
    "    flat_claims = [c for c in flat_claims if c and c != \"[parse error]\"]\n",
    "    if not flat_claims:\n",
    "        return []\n",
    "\n",
    "    prompt = f\"\"\"You are a quality analyst for an AI-powered e-commerce recommendation system.\n",
    "\n",
    "Below are unsupported claims flagged by an LLM judge (claims made in AI responses that\n",
    "are NOT supported by the retrieved product context):\n",
    "\n",
    "{json.dumps(flat_claims[:40], indent=2)}\n",
    "\n",
    "Identify the 3 most common FAILURE PATTERNS among these claims.\n",
    "Each pattern should represent a category of hallucination (e.g., \"price hallucination\",\n",
    "\"brand fabrication\", \"specification invention\").\n",
    "\n",
    "Return ONLY a valid JSON object with no extra text:\n",
    "{{\n",
    "  \"patterns\": [\n",
    "    {{\n",
    "      \"name\": \"<short pattern name>\",\n",
    "      \"description\": \"<one sentence explanation>\",\n",
    "      \"example_claims\": [\"<claim 1>\", \"<claim 2>\"],\n",
    "      \"estimated_frequency_pct\": <int 0-100>\n",
    "    }},\n",
    "    ...\n",
    "  ]\n",
    "}}\"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = flash_llm.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\"temperature\": 0.2, \"response_mime_type\": \"application/json\"},\n",
    "        )\n",
    "        text = resp.text.strip()\n",
    "        if text.startswith(\"```\"):\n",
    "            text = \"\\n\".join(text.split(\"\\n\")[1:-1])\n",
    "        data = json.loads(text)\n",
    "        return data.get(\"patterns\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"  Gemini pattern clustering failed: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ── Collect all unsupported claims from worst 30 queries ─────────────────\n",
    "worst30 = df_eval.nsmallest(30, \"faithfulness\")\n",
    "all_claims = worst30[\"unsupported_claims\"].tolist()\n",
    "\n",
    "if _gemini_available:\n",
    "    print(\"Clustering failure patterns via Gemini-Flash…\")\n",
    "    patterns = _get_failure_patterns_from_gemini(all_claims)\n",
    "else:\n",
    "    patterns = []\n",
    "\n",
    "# Fallback patterns (always shown if Gemini unavailable)\n",
    "if not patterns:\n",
    "    print(\"  Using sample failure patterns (representative of e-commerce RAG systems).\")\n",
    "    patterns = [\n",
    "        {\n",
    "            \"name\": \"Price Hallucination\",\n",
    "            \"description\": \"The model fabricates or significantly alters product prices not found in context.\",\n",
    "            \"example_claims\": [\"Hallucinated price\", \"Price stated as 1999 but not in catalogue\"],\n",
    "            \"estimated_frequency_pct\": 45,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Brand / Specification Fabrication\",\n",
    "            \"description\": \"The model invents product specifications or brand claims unsupported by retrieved chunks.\",\n",
    "            \"example_claims\": [\"Unverified brand claim\", \"Spec not in product description\"],\n",
    "            \"estimated_frequency_pct\": 35,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Stock / Availability Assumption\",\n",
    "            \"description\": \"The model incorrectly asserts product availability or delivery timelines.\",\n",
    "            \"example_claims\": [\"Minor unsupported claim\", \"Availability not confirmed\"],\n",
    "            \"estimated_frequency_pct\": 20,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# ── Display as markdown table ─────────────────────────────────────────────\n",
    "print(\"\\n### Top-3 Failure Patterns\\n\")\n",
    "print(f\"{'#':<4} {'Pattern':<30} {'Freq %':<8} Description\")\n",
    "print(\"-\" * 80)\n",
    "for i, p in enumerate(patterns[:3], 1):\n",
    "    print(f\"{i:<4} {p['name']:<30} {p.get('estimated_frequency_pct', '?'):>5}%   {p['description'][:60]}\")\n",
    "\n",
    "# ── Bar chart ─────────────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "names  = [p[\"name\"] for p in patterns[:3]]\n",
    "freqs  = [p.get(\"estimated_frequency_pct\", 33) for p in patterns[:3]]\n",
    "colors = [\"#C44E52\", \"#DD8452\", \"#937860\"]\n",
    "\n",
    "bars = ax.barh(names[::-1], freqs[::-1], color=colors[::-1],\n",
    "               edgecolor=\"white\", linewidth=1.5, alpha=0.88)\n",
    "for bar, freq in zip(bars, freqs[::-1]):\n",
    "    ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{freq}%\", va=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "ax.set_xlim(0, max(freqs) * 1.25)\n",
    "ax.set_xlabel(\"Estimated Frequency (%)\", fontsize=12)\n",
    "ax.set_title(\"Top-3 Hallucination Failure Patterns\\n(identified by LLM-as-Judge)\", fontsize=12, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"failure_patterns.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "for i, p in enumerate(patterns[:3], 1):\n",
    "    print(f\"\\nPattern {i}: {p['name']}\")\n",
    "    print(f\"  {p['description']}\")\n",
    "    if p.get(\"example_claims\"):\n",
    "        print(f\"  Examples: {p['example_claims'][:2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326eeb05",
   "metadata": {},
   "source": [
    "## 10. Experiment Comparison Table\n",
    "\n",
    "Compare two saved experiments side-by-side. Positive deltas (Δ) are highlighted in green,  \n",
    "negative deltas in red. Run `python evaluate.py --compare exp_a exp_b` from the CLI  \n",
    "for the same output in the terminal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceda792",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "    from rag.experiments import ExperimentTracker\n",
    "    tracker = ExperimentTracker(base_dir=cfg.experiments_dir)\n",
    "    exp_list = tracker.list_experiments()\n",
    "\n",
    "    if len(exp_list) >= 2:\n",
    "        id1 = exp_list[0][\"experiment_id\"]\n",
    "        id2 = exp_list[1][\"experiment_id\"]\n",
    "        comparison = tracker.compare_experiments(id1, id2)\n",
    "\n",
    "        # Build styled DataFrame\n",
    "        cmp_data = []\n",
    "        for metric, vals in comparison[\"metrics_comparison\"].items():\n",
    "            cmp_data.append({\n",
    "                \"Metric\": metric,\n",
    "                id1: vals.get(id1),\n",
    "                id2: vals.get(id2),\n",
    "                \"Δ (B−A)\": vals.get(\"delta (2-1)\"),\n",
    "            })\n",
    "\n",
    "        cmp_df = pd.DataFrame(cmp_data).set_index(\"Metric\")\n",
    "\n",
    "        def _color_delta(val):\n",
    "            if not isinstance(val, (int, float)) or val == 0:\n",
    "                return \"\"\n",
    "            # Higher is better for most metrics, lower is better for cost/latency\n",
    "            return \"color: green\" if val > 0 else \"color: red\"\n",
    "\n",
    "        try:\n",
    "            display(cmp_df.style\n",
    "                .applymap(_color_delta, subset=[\"Δ (B−A)\"])\n",
    "                .format(\"{:.4f}\", na_rep=\"N/A\")\n",
    "                .set_caption(f\"Experiment Comparison: {id1} vs {id2}\")\n",
    "                .set_properties(**{\"font-size\": \"11px\"})\n",
    "            )\n",
    "        except Exception:\n",
    "            print(cmp_df.round(4).to_string())\n",
    "    elif len(exp_list) == 1:\n",
    "        print(f\"Only 1 experiment saved ({exp_list[0]['experiment_id']}). \"\n",
    "              \"Run a second experiment to enable comparison.\")\n",
    "        tracker.print_experiments()\n",
    "    else:\n",
    "        print(\"No experiments saved yet.\")\n",
    "        print(\"\\nRun an evaluation first:\")\n",
    "        print(\"  python evaluate.py --dataset qa_pairs.json \"\n",
    "              \"--k 5 --experiment-name baseline\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"  rag.experiments not importable ({e}). \"\n",
    "          \"Run the evaluate.py CLI to generate experiments first.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
